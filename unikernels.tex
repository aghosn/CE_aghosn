\section{Unikernels: Library Operating Systems for the Cloud}
Traditionally, applications are deployed in the Cloud as virtual machines (VM).
A VM packs a complete operating system, such as Linux or Windows, that allows to run unmodified application processes.
Typically, a VM deployed in the Cloud can be viewed as a single-purpose appliance, i.e., it consists of a guest operating system running a main application, e.g., a database or web server, that relies on smaller services running in parallel.
Due to the lack of standards for application configuration, VMs often run custom scripts as glue code to initialize the execution environment.

Although destined to execute a single main application, Cloud VMs are insufficiently specialized.
They contain an entire operating system image, distributed with some default services.
Apart from unnecessarily increasing the disk image size for this single-purpose appliance, hence requiring more resources to deploy and run the VM, the presence of unwanted default services increases the potential attack surface exposed to a malicious entity and threatens the integrity of the VM.

The Unikernel paper[CITE] presents a new model to deploy applications in the Cloud that addresses the above-mentioned issues by radically specializing the Cloud application's image.
\emph{Unikernels} are small sized sealed single-purpose appliances entirely written in a high-level programming language, that can easily be configured and deployed over Cloud platforms.


\subsection{Design Principles}
The Unikernel revisists the idea of library operating systems (libOS) by applying it to Cloud platforms.
A unikernel is a standalone kernel that encapsulates a single-purpose application destined to be deployed over the Cloud.
As such, unikernels execute on top of a VM hypervisor, e.g., Xen[CITE].
The advantages of targeting a virtualization platform instead of a custom kernel are twofold.
First, the hypervisor provides a fixed virtual hardware abstraction that alleviates the need to take into account heterogeneous hardware devices.
The hypervisor provides hardware drivers for a large number of devices and abstracts them away behind a fixed high-level interface.
Second, the hypervisor virtual abstraction facilitates both vertical and horizontal dynamic scaling.

Unikernels achieve aggressive specialization by relying on a single high-level programming language.
System libraries and the application are implemented in a common, statically typed, programming language, and compiled as a single specialized unikernel.
The compiler performs static analysis on the entire user stack and optimizes the whole system at once.
This enables to:
\begin{enumerate*}
	\item perform most of the appliance static configuration at compile time,
	\item reduce the final image's size, and
	\item improve the application's security
\end{enumerate*}, all of which without sacrificing performance.

Static configuration of a unikernel appliance happens at compile time.
As mentioned previously, standard VMs incur a non-negligible deployment overhead.
Custom scripts are executed to configure and start all services needed by the application, e.g., a database for a web server, and glue together independent packages or applications.
Unikernels, on the other hand, integrate all dependencies as libraries, compiled within the application itself.
Build parameters passed to the compilation configuration enable static configuration, while dynamic configuration is achieved via specific function calls to the library.

Unikernels are highly specialized and compact binaries.
Since all the libraries are linked and compiled against the application itself, static analysis techniques such as dead-code elimination (DCE) can be applied to reduce the final image's size.
The authors claim that in most cases, the final binary's size is on the order of kilobytes, and hence easily deployable across the Internet.
Furthermore, in the context of Cloud platforms, where resources are rented, small size binaries consume less resources to host the program's code and data, and hence minimize expenses.

By eschewing backward compatibility and requiring the entire stack to be written in a single high-level typed programming language, unikernels can leverage type-safety as well as static analysis and verification tools to improve the application's security.
First, DCE reduces the attack surface exposed to a remote attacker system-wide.
Unused libraries as well as unreachable portions of code are not included in the final binary and cannot be leveraged by an attacker to subvert the application.
Second, a unikernel executes entirely within a single address space and relies on the language's type safety to enforce access control and restrictions.
The single-address space model eases the integration of security techniques. 
For example, to prevent code injection, the unikernel is initialized such that no page mapping is both executable and writable.
To further protect against possible attacks, the unikernel can issue a \emph{seal} hypercall, that prevents future page table modifications.
Other techniques and code instrumentation, e.g., address space randomization, stack canaries and guard pages, can also easily be added.

While type safety and system-wide optimizations are attractive, unikernels represent a daunting engineering challenge.
All libraries and protocols used by the application need to be rewritten in the same high-level language.
An alternate solution is to leverage interoperability at the network level.
Services that are not reimplemented in the chosen language can be encapsulated in separate VMs and communicate with the unikernel via standard network protocols.
Unfortunately, this technique increases resource usage and prevents system-wide optimizations.
Finally, we note that parts of the system, such as the garbage collector, are type-unsafe and still require to be protected against. 

A common argument against type-safe and memory managed programming languages is the overhead incurred by type checks and garbage collection.
Performance is critical to Cloud services renters to achieve their service level objectives.
According to the authors, unikernels performances are comparable to standard VMs.
We report the evaluation results presented in [CITE] in the next section.

\subsection{Prototype and Evaluation}
%implementation called Mirage.
%Implemented in OCaml
%Hypervisor is Xen.
%Single process, threading achieved via lwthreads part of the language.
%typed protocol I/O give details from p465.
%Language runtime slightly modify, special support for device drivers based on I/O (not sure if going to talk about it).
%
%Evaluation performs microbenchmarks, DNS Server for safe network stack, Openflow control appliance, web server and database (storing and networking), and respective sizes.
%
%Boot time: less than half time required by linux debian running apache 2 in paravirtualized mode.
%Modifying how Xen builds domains enables to reboot the machine in less than 50 milliseconds.
%
%threads creation: better than pv and native. Creating 20 millions is more than twice faster in mirage os than pv, and noticeably faster than native. heap allocation.
%also more precised thread timers, mostly due to no user/kernel space crossings
%
%Net + Storage
% 10% overhead for mirage due to type safety.
% TCPv4 comparison on par with Linux.
%\adrien{Maybe sum it up better, say that microbenchmarks prove that no performance overhead is introduced.
%Then say general comment about the real life applications, and say we focus on bind because it shows two things: improved perf, and better security due to type-safety.}
%The authors evaluate a prototype evaluation of unikernels, called Mirage, that leverages OCaml system programming and static type system to produce appliances running on top of a Xen[CITE] hypervisor.
%A Mirage application is a standalone kernel, encapsulating a single process running in a 64-bit address space.
%Concurrency within a VM is achieved via the OCaml Lwt cooperative threading[CITE] library.
%At the same time, a \emph{domainpoll} function enables to block the VM for external events and timeouts.
%Lightweight threads enable to implement I/O protocols in a type-safe, non-blocking manner.
%Network processing as well as storage I/O are implemented as unikernel libraries under the control of the application that rely on a unified device driver model to read/write from/to a device.
%Mirage applications contain a guest VM driver implemented in OCaml that communicates with a Xen backend driver via events and a shared memory page that holds requests and responses for device operations.
%This shared memory ring is at the heart of every I/O in Mirage and allows device drivers to be implemented as OCaml libraries linked against the application.
%The Mirage runtime memory management was specialized to dedicate segment to these external I/O pages.
%
%Mirage's performance evaluation consists in microbenchmarks, to evaluate standalone features such as boot time and the network stack, and more realistic applications, namely a DNS server, and Openflow appliance, and a web server %backed by a database.
%We briefly report the results from[CITE].
%
%Mirage appliances exhibit satisfactory performance results in the microbenchmark evaluation.
%The Mirage image time to boot is more than twice smaller than an equivalent Linux kernel running as a paravirtualised Xen domU executing a similar application.
%Regarding thread creation, Mirage manages to create 20 millions of threads more than twice faster than the paravirtualized linux, and slightly faster than linux running natively.
%Surprisingly, the single address space model used by Mirage allows less jitter in thread timers than linux-based configurations, mostly due to the absence of user/kernel crossings.
%Next, the evaluation of the TCPv4 stack implementation in Mirage shows that it performs similarly to the Linux 3.7 TCPv4 one, while providing a type-safe implementation of the protocol.
%Finally, Mirage performs the same as a the paravirtualized linux with direct I/O (no buffer) for a simple random read throughput test.
%
%For more realistic applications, Mirage's performance is at least equivalent (and sometimes superior) to state-of-start implementations of the same functionalities.
%The Mirage DNS Server appliance achieves 75-80 kqueries/s, while NSDN, a high performance implementation, reaches a max throughput of 70 kqueries/s.
%Apart from an increased throughput, Mirage provides a type-safe implementation that is not amenable to most of the past 10 years reported bugs in the Bind software.\\
%\adrien{Re-writing}
The authors evaluate a prototype unikernel implementation, called Mirage, that leverages OCaml system programming and static type system to produce appliances running on top of a Xen[CITE] hypervisor.
A Mirage application is a standalone kernel, encapsulating a single process running in a 64-bit address space.
Concurrency within a VM is achieved via the OCaml Lwt cooperative threading[CITE] library.
At the same time, a \emph{domainpoll} function enables to block the VM for external events and timeouts.
Lightweight threads enable to implement I/O protocols in a type-safe, non-blocking manner.
Network processing as well as storage I/Os are implemented as unikernel libraries under the control of the application and rely on a unified device driver model to read/write from/to a device.
Mirage applications contain a guest VM driver implemented in OCaml that communicates with a Xen backend driver via events and a shared memory page that holds requests and responses for device operations.
This shared memory ring is at the heart of every I/O in Mirage and allows device drivers to be implemented as OCaml libraries linked against the application.
The Mirage runtime memory management was specialized to dedicate a segment to these external I/O pages.

Mirage main features are evaluated against a set of microbenchmarks.
Compared to a Linux kernel running as a paravirtualized Xen domU, a Mirage appliance boots more than two times faster, enables the creation of 20 millions threads twice as fast, incurs less jitter in thread timers thanks to the absence of user/kernel crossings, and performs network and storage I/O without significant overheads.

Mirage is then evaluated on more realistic applications, namely, a DNS server, an OpenFlow controller appliance, and a web server backed by a database.
In the remainder of this section, we focus on the DNS server evaluation that exhibits most advantages of relying on Mirage to deploy Cloud applications.
We note, however, that in all experiments, Mirage performance is close to (or even better than) state-of-the-art implementations.

The Mirage DNS server application includes libraries for the network stack and an in-memory filesystem.
The authors compare the Mirage appliance to a state-of-the-art high performance implementation, NSD 3.2.10.
Adding memorization of responses enabled the Mirage appliance to reach a throughput lying between 75 and 80 kqueries/s.
In comparison, NSD reaches a maximal throughput of 70 kqueries/s.
Apart from the performance evaluation, the authors highlight the benefits of having a type-safe implementation of a DNS server.
Reportedly, the Bind software (a mature and popular DNS server) suffered from 40 important vulnerabilities in the past 10 years, mostly related to memory management errors and poor handling of exceptional cases.
Mirage's type-safety prevents this kind of errors by design.
The DNS server Mirage appliance is a perfect example showing that type-safety, and more generally improved security, do not  preclude performance. 

To conclude, as promised, Mirage yields small size binary images, even for realistic applications.
For example, the Mirage DNS server image is 183.5 kB, while the equivalent Linux image, striped down to the used components, is 462 MB.


\subsection{Discussion}

\input{unikernels_discussion}