Modern applications for desktop PCs, mobile devices, and even web services incorporate multiple layers of software frameworks, interface with public APIs, and rely on external libraries, all of which might come from various, potentially untrusted, sources.
As a consequence of their intrinsic heterogeneity, most applications cannot be trusted and require strong isolation to prevent them from corrupting their host, or impacting other services running on the same system.
Sandboxing mechanisms are therefore an essential building block of today's systems: Cloud services run guest applications within containers or virtual machines, web browsers sandbox the JavasSript and web-applets executed while visiting web pages, and operating systems run untrusted applications in unprivileged mode.
In this classical hierarchical security model, the host is part of the trusted privileged software stack, and must be protected from the untrusted guest (application).
In other words, common sandboxes implement a single-sided isolation mechanism.
This model, unfortunately, presents two main concerns 
\begin{enumerate*}
	\item sandboxing introduces overheads that impact the application's performance
	\item due to the new ways in which applications are shared and deployed, as well as the increasing risk of having a compromised host, guest applications require mechanisms to prevent privileged code from accessing their protected sensitive data
\end{enumerate*}.


\adrien{Performance rigid APIs, kernel-by-pass etc., adding sandboxing is often very limiting}
\adrien{Design to reconciliate performance and security}
\adrien{Description of sandboxing}
\adrien{Unfortunately two main concerns: not bidirectional, and impact performance too}
Cloud or Microsoft or CIA



Applications and standardized operating systems abstractions evolved at different paces over the past decades, leading to an important mismatch that impedes both the security and performance of modern software.
Application developers require ever-higher network throughputs, finer-grained memory management, access to various devices, increasingly demanding isolation guarantees, all of which is unachievable with the common standards (e.g., POSIX) defined decades ago.
The rigidity of existing operating systems APIs and system services implementations lead to the appearance of ad-hoc solutions, that strive to provide applications with either more flexible or better specialized implementations.
For example, such solutions include the re-implementations of the thread abstraction[CITE], IPC mechanisms[CITE], memory abstractions[CITE], or the networking stack[CITE IX and user-level impl].

Traditional operating system designs lack the flexibility necessary to address modern security and privacy concerns.


Modern applications for desktop PCs, mobile devices, and even web services incorporate multiple layers of software frameworks, interface with public APIs, and rely on external libraries, all of which might come from various, potentially untrusted, sources.
As a consequence of their intrinsic heterogeneity, most applications cannot be trusted and require strong isolation to prevent them from corrupting their host, or impacting other services running on the same system.
Sandboxing mechanisms are therefore an essential building block of today's systems: Cloud services run guest applications within containers or virtual machines, web browsers sandbox the JavasSript and web-applets executed while visiting web pages, and operating systems run untrusted applications in unprivileged mode.
Regardless of the way the application is deployed (e.g., on a cloud service, on a personal computer etc.), the adopted hierarchical security model aims to protect a trusted privileged host from an untrusted guest.
This model, however, seems obsolete.
Recent evolutions in the way applications are deployed (e.g., Cloud services), as well as scandals revealing mainstream operating system's aggressive aggregation of user data, seems to suggest that the host cannot be trusted by the application.
In the light of these observations, 

Aggregation of data, increase of malware.

Meanwhile, the strive for performance is still present, applications need to process ever increasing amounts of data in a timely fashion.


In this classical hierarchical security model, the host is part of the trusted privileged software stack, and must be protected from the untrusted guest (application).
In other words, common sandboxes implement a single-sided isolation mechanism.
This model, unfortunately, presents two main concerns with regards to modern applications' requirements
\begin{enumerate*}
	\item sandboxing limits the application's performance by fixing rigid APIs and the implementation of underlying system services
	\item modern applications generate and process sensitive data that must be protected from a compromised or flawed host
\end{enumerate*}.
We successively develop both of these concerns in the following paragraphs.

Traditional operating systems

Even push further, provided abstractions do not work.


\adrien{Performance rigid APIs, kernel-by-pass etc., adding sandboxing is often very limiting}
\adrien{Design to reconciliate performance and security}
\adrien{Description of sandboxing}
\adrien{Unfortunately two main concerns: not bidirectional, and impact performance too}
Cloud or Microsoft or CIA


%%The advent of the Internet area lead to new application deployment paradigms and an incredible increase in the amount of sensitive user data transiting throughout the web.
%%Cloud service providers enable even small companies to quickly deploy web-applications at a low starting cost.
%%Applications from multiple, mutually distrusted sources, are co-located on the same machine.


\subsection{Operating systems did not}
\adrien{Use the paper sent by Marios for everything that is related to POSIX abstractions and new layered application.
shift of paradigm where applications mostly interface with frameworks that either interact with os or by-pass it to re-implement some low-level features such as network.}

Meanwhile, operating system abstractions did not evolve.
Unix was designed in the seventeens, and is a source of inspiration for the current operating systems.
At the same time, it's the main source of inspiration for POSIX.
Problem is, these abstraction do not take into account new challenges.
That's why some papers break POSIX compatibility, or simply by-pass the kernel.
NEED EXAMPLES (IX can be one for dataplanes, LWC for more flexible memory management).\\

Kernel is responsible for resource protection and management at the same time.
While protection makes sense, i.e., role is to share the resources and ensure each unit some amount,
management seems to be bad, because the kernel is not aware of what the application needs.
A good example is garbage collection (okay I need to find a paper about this, maybe in the exokernel).\\

Finally kernels are all powerful. A compromised kernel can hurt any application.
While denial of service seems unavoidable, what might be a problem is data leakage, corruption, or highjacking of an application.
With more an more co-located applications on cloud services, this is a real problem.
Past efforts were made to protect host from guest, but now we actually want some guarantees the other way around as well.\\

\textit{Summary}: kernel abstractions did not follow the same evolution as applications.
There's a requirement for more liberty in resource management for the application, we want to separate protection from management.
At the same time, we also need to protect guest from host.

\subsection{Objectives}
 In this paper, we want to study existing designs that could better answer today's requirements imposed by the way applications are developped and deployed.
 We want the kernel to focus on protection rather than management.
 The application's knows best how to manage its resources, and should therefore be given access to such resources to do with as they want.
 By leaving the application with most of the management, we de-involve the kernel and hence get a better separation between kern and application, that allows to more easily provide a symmetric isolation/protection mechanisms from host to guest and guest to host.
 We also want to study how modern software tools and techniques can impact kernel design.
 How can we leverage software verification, static analysis and all of that in kernels? 
 C and C++ are the most used languages, we might want a completely type safe language.\\

 \textit{Summary:}This requires modifictions at several levels.
 We study a hardware solution, a kernel one, and a software one.
 We first present the following papers, and then the research proposal.
%Past decades have seen the appearance of new applications.
%Available online, spread everywhere, frameworks, libraries.
%Multiple source give code that makes current applications ALSO dynamically download code in browser.
%This means new security challenges, with fine-grained control over what's happening.
%At the same time, explosion of data that circulates, requirement for performance.
%Finally, deployed in new ways. Not only running on a personal hardware, but deployed on cloud services, co-located with other applications.
%Still requirement for security, isolation, non-trust of the host, performance.\\


%In the past decades, software applications have multiplied, became more complex, and intertwined in new ways.%
%A vast majority of modern applications rely on public API's, external libraries, or software frameworks, hence incorporating code from various, someti%mes untrusted, sources.%
%The web has become the main distribution platform for such software, hence making it impractically hard to verify sources. %TODO reformulate this one.%
%\adrien{Rephrase all of that}The emergence and explosion of such complex systems leads to new security challenges, as verification tools do not allow to ensure a bug free software, applications still need to be executed without being %trusted.
%At the same time, 
%\adrien{Not sure if should keep it or just forget about it}Targeting a framework is much more valuable for attackers as it can potentially affect a large number of applications.
%
%Unavoidable e.g., browsers require to execute javascript, run untrusted web applets etc.
%Browsers need to rely on sandboxing mechanisms to isolate javascript code ran on each page, and download and execute untrusted web applets.
%Another mechanism implemented by certain operating systems consists in maintaining a list of trusted software providers and warn users whenever code from an untrusted source is being executed (Microsoft mitigation mechanism).
%
%Not only did the way applications were developed and distributed changed, but also the way to deploy them.
%Cloud services, for example, host and co-located applications that originate from different sources.
%This new way to deploy applications requires to isolated untrusted applications and prevent them from harming both the underlying host and other co-located applications.
%Still requires to trust the host.
%
%\adrien{Performance???}


%Cloud computing providers enable even small organizations to deploy web-based services quickly, with low start-up costs, and efficiently adapt the amount of available resources to their current load.
%On a machine, the cloud service provider's host divides physical resources among co-located applications from different origins.
%While very attractive for their simplicity and adaptability, such services raise important security concerns.
%
%From the service provider's point of view, co-locating mutually distrusted applications requires to
%\begin{enumerate*}
%	\item isolate applications from each other and
%	\item prevent an application from corrupting the host
%\end{enumerate*}.
%\adrien{Take things from Haven}.
%As a result, sandboxing mechanisms became a fundamental building block of today's cloud services.
%These mechanisms often follow a classical hierarchical security model with a single-sided isolation mechanism, i.e., privileged trusted code (the host) is protected from the untrusted one (the guest) while retaining access to all the application's data.
%
%From the guest application's point of view, the lack of bidirectional isolation requires to either treat the entire host privileged software stack as a trusted part of the execution, or develop mitigation solutions (e.g., operate on encrypted data) to protect some %parts of the user data from the host.%
%
%While obvious for Cloud computing, such concerns can be generalized to standard computing systems?
%
%\adrien{Pursue with the need for performance.}
%
%\adrien{At the same time, it is not exactly the same as having access to your own hardware.
%The performance is tunable to the extent of the provided service abstractions e.g. bare metal, hybrid or hosted, but with no real control of what is done by the host. Here I should argue more about the performance and the form taken by applications?}
%
%\textit{Summary}: Applications more and more complex, deployed in new ways, require performance, 